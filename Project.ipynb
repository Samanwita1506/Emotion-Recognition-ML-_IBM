{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad79ebaf-378a-4d51-a3fb-5b61a7566ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfc3854f-56b0-42d1-9fbb-6f2bcf6a2f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Samanwita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Samanwita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Samanwita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# text preprocessing\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0f753b1-31c7-4ef8-9907-a2752b0600d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (15999, 2)\n",
      "\n",
      "First 5 rows:\n",
      "                                                text  emotion\n",
      "0  i can go from feeling so hopeless to so damned...  sadness\n",
      "1   im grabbing a minute to post i feel greedy wrong    anger\n",
      "2  i am ever feeling nostalgic about the fireplac...     love\n",
      "3                               i am feeling grouchy    anger\n",
      "4  ive been feeling a little burdened lately wasn...  sadness\n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15999 entries, 0 to 15998\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   text     15999 non-null  object\n",
      " 1   emotion  15999 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 250.1+ KB\n",
      "\n",
      "Missing Values:\n",
      "text       0\n",
      "emotion    0\n",
      "dtype: int64\n",
      "\n",
      "Emotion Distribution:\n",
      "emotion\n",
      "joy         5362\n",
      "sadness     4665\n",
      "anger       2159\n",
      "fear        1937\n",
      "love        1304\n",
      "surprise     572\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Emotion Percentages:\n",
      "emotion\n",
      "joy         33.514595\n",
      "sadness     29.158072\n",
      "anger       13.494593\n",
      "fear        12.107007\n",
      "love         8.150509\n",
      "surprise     3.575223\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "df = pd.read_csv(\"emotion_dataset.csv\")\n",
    "\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nDataset Info:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nEmotion Distribution:\")\n",
    "print(df['emotion'].value_counts())\n",
    "print(\"\\nEmotion Percentages:\")\n",
    "print(df['emotion'].value_counts(normalize=True) * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9950d97-009d-4bb8-9064-b5654cd2d193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Cleaning Complete!\n",
      "\n",
      "Example:\n",
      "Original: i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake\n",
      "Cleaned: go feeling hopeless damned hopeful around someone care awake\n"
     ]
    }
   ],
   "source": [
    "# Initializing lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Keeping the important emotion-related words\n",
    "emotion_words = {'not', 'no', 'never', 'missing', 'miss', 'sad', 'happy', \n",
    "                 'angry', 'scared', 'fear', 'love', 'hate', 'joy', 'worry'}\n",
    "stop_words = stop_words - emotion_words\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Enhanced text cleaning with stopword removal and lemmatization\"\"\"\n",
    "    # Converting to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Removing the URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    \n",
    "    # Removing the numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    \n",
    "    # Removing the punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    # Removing the extra spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    # Tokenizing and removing stopwords\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "df['clean_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "print(\"Text Cleaning Complete!\")\n",
    "print(\"\\nExample:\")\n",
    "print(\"Original:\", df['text'].iloc[0])\n",
    "print(\"Cleaned:\", df['clean_text'].iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a980ddf-e5c4-422d-a1ad-dfac0f525ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion Labels: ['anger' 'fear' 'joy' 'love' 'sadness' 'surprise']\n",
      "Encoded Labels: [0 1 2 3 4 5]\n",
      "\n",
      "Training samples: 12799\n",
      "Testing samples: 3200\n",
      "\n",
      "Training set emotion distribution:\n",
      "  anger: 1727\n",
      "  fear: 1550\n",
      "  joy: 4289\n",
      "  love: 1043\n",
      "  sadness: 3732\n",
      "  surprise: 458\n"
     ]
    }
   ],
   "source": [
    "# Features and labels are used\n",
    "X = df['clean_text']\n",
    "y = df['emotion']\n",
    "\n",
    "#  labels are encoded\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "print(\"Emotion Labels:\", le.classes_)\n",
    "print(\"Encoded Labels:\", np.unique(y_encoded))\n",
    "\n",
    "# the dataset is split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y_encoded  # Ensuring balanced distribution\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining samples: {len(X_train)}\")\n",
    "print(f\"Testing samples: {len(X_test)}\")\n",
    "\n",
    "# checking the distribution\n",
    "print(\"\\nTraining set emotion distribution:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for emotion, count in zip(le.classes_, counts):\n",
    "    print(f\"  {emotion}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36bde0fe-8183-429b-a4dd-17bbf6b25a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectorization Complete!\n",
      "Training features shape: (12799, 5000)\n",
      "Testing features shape: (3200, 5000)\n",
      "\n",
      "Top 10 features: ['abandoned' 'abc' 'ability' 'able' 'able feel' 'able find' 'able get'\n",
      " 'able help' 'able move' 'able share']\n"
     ]
    }
   ],
   "source": [
    "# Bigrams and filtering enhance the TF-IDF representation\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,     \n",
    "    ngram_range=(1, 2),    \n",
    "    min_df=2,               \n",
    "    max_df=0.95,            \n",
    "    sublinear_tf=True       \n",
    ")\n",
    "\n",
    "# The system transforms the text into TF-IDF features\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(\"TF-IDF Vectorization Complete!\")\n",
    "print(f\"Training features shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Testing features shape: {X_test_tfidf.shape}\")\n",
    "print(f\"\\nTop 10 features: {tfidf.get_feature_names_out()[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31003ffa-9125-4c16-bb42-281abb1a10bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "✓ Model training completed!\n"
     ]
    }
   ],
   "source": [
    "#Class balancing is applied while training the Logistic Regression model\n",
    "model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced',  # Handles imbalanced classes\n",
    "    random_state=42,\n",
    "    solver='lbfgs'\n",
    ")\n",
    "\n",
    "print(\"Training model...\")\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "print(\"✓ Model training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42642e68-200c-49cc-a9ab-2ed3990febb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.8962 (89.62%)\n",
      "\n",
      "============================================================\n",
      "CLASSIFICATION REPORT\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.89      0.89      0.89       432\n",
      "        fear       0.89      0.85      0.87       387\n",
      "         joy       0.94      0.88      0.91      1073\n",
      "        love       0.73      0.93      0.81       261\n",
      "     sadness       0.95      0.92      0.94       933\n",
      "    surprise       0.67      0.92      0.78       114\n",
      "\n",
      "    accuracy                           0.90      3200\n",
      "   macro avg       0.85      0.90      0.87      3200\n",
      "weighted avg       0.90      0.90      0.90      3200\n",
      "\n",
      "\n",
      "============================================================\n",
      "CONFUSION MATRIX\n",
      "============================================================\n",
      "          anger  fear  joy  love  sadness  surprise\n",
      "anger       383     9   15     5       19         1\n",
      "fear         10   329    8     1        9        30\n",
      "joy          12     3  949    81       17        11\n",
      "love          3     2   13   242        1         0\n",
      "sadness      23    17   20     4      860         9\n",
      "surprise      0     8    1     0        0       105\n"
     ]
    }
   ],
   "source": [
    "# Makeing predictions\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# The algorithm measures the accuracy.\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {acc:.4f} ({acc*100:.2f}%)\")\n",
    "\n",
    "# generating a details for classification report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "# The confusion matrix is generated\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(\"=\"*60)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_df = pd.DataFrame(cm, index=le.classes_, columns=le.classes_)\n",
    "print(cm_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ada94a9-c0c1-45d9-919c-d8ec938c1a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Predictions:\n",
      "============================================================\n",
      "Text: I am feeling very happy today!\n",
      "Predicted Emotion: joy (Confidence: 70.17%)\n",
      "------------------------------------------------------------\n",
      "Text: I am missing you\n",
      "Predicted Emotion: sadness (Confidence: 30.28%)\n",
      "------------------------------------------------------------\n",
      "Text: This makes me so angry\n",
      "Predicted Emotion: anger (Confidence: 90.29%)\n",
      "------------------------------------------------------------\n",
      "Text: I'm scared about the exam\n",
      "Predicted Emotion: fear (Confidence: 89.05%)\n",
      "------------------------------------------------------------\n",
      "Text: I love spending time with you\n",
      "Predicted Emotion: love (Confidence: 31.62%)\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def predict_emotion(text):\n",
    "    \"\"\"Predict emotion for any text input\"\"\"\n",
    "    # The text is cleaned before processing.\n",
    "    cleaned = clean_text(text)\n",
    "    \n",
    "    # The input is converted into TF-IDF features\n",
    "    vector = tfidf.transform([cleaned])\n",
    "    \n",
    "    #Predictions are made.\n",
    "    pred = model.predict(vector)[0]\n",
    "    label = le.inverse_transform([pred])[0]\n",
    "    \n",
    "    # The prediction probabilities are produced by the model\n",
    "    proba = model.predict_proba(vector)[0]\n",
    "    confidence = max(proba) * 100\n",
    "    \n",
    "    return label, confidence\n",
    "\n",
    "# The function is tested.\n",
    "test_examples = [\n",
    "    \"I am feeling very happy today!\",\n",
    "    \"I am missing you\",\n",
    "    \"This makes me so angry\",\n",
    "    \"I'm scared about the exam\",\n",
    "    \"I love spending time with you\"\n",
    "]\n",
    "\n",
    "print(\"Testing Predictions:\")\n",
    "print(\"=\"*60)\n",
    "for text in test_examples:\n",
    "    emotion, confidence = predict_emotion(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predicted Emotion: {emotion} (Confidence: {confidence:.2f}%)\")\n",
    "    print(\"-\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40ceda2f-b621-4f48-9654-641fe3cfb9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All models saved successfully!\n",
      "\n",
      "Saved files:\n",
      "  - emotion_model.joblib\n",
      "  - tfidf_vectorizer.joblib\n",
      "  - label_encoder.joblib\n"
     ]
    }
   ],
   "source": [
    "#All components are saved\n",
    "joblib.dump(model, \"emotion_model.joblib\")\n",
    "joblib.dump(tfidf, \"tfidf_vectorizer.joblib\")\n",
    "joblib.dump(le, \"label_encoder.joblib\")\n",
    "\n",
    "print(\"✓ All models saved successfully!\")\n",
    "print(\"\\nSaved files:\")\n",
    "print(\"  - emotion_model.joblib\")\n",
    "print(\"  - tfidf_vectorizer.joblib\")\n",
    "print(\"  - label_encoder.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1e58495-a3a7-4322-beee-dccf3caea567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# The saved models are loaded\n",
    "loaded_model = joblib.load(\"emotion_model.joblib\")\n",
    "loaded_tfidf = joblib.load(\"tfidf_vectorizer.joblib\")\n",
    "loaded_le = joblib.load(\"label_encoder.joblib\")\n",
    "\n",
    "print(\"✓ Models loaded successfully!\")\n",
    "\n",
    "# Testing is conducted with user examples\n",
    "def predict_with_loaded_model(text):\n",
    "    cleaned = clean_text(text)\n",
    "    vector = loaded_tfidf.transform([cleaned])\n",
    "    pred = loaded_model.predict(vector)[0]\n",
    "    label = loaded_le.inverse_transform([pred])[0]\n",
    "    proba = loaded_model.predict_proba(vector)[0]\n",
    "    confidence = max(proba) * 100\n",
    "    return label, confidence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e878f28e-99f4-4e55-91a8-94a8bb4dd81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test: 'I am with my family'\n",
      "Predicted Emotion: joy\n",
      "Confidence: 30.58%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_sentence = \"I am with my family\"\n",
    "emotion, conf = predict_with_loaded_model(test_sentence)\n",
    "print(f\"\\nTest: '{test_sentence}'\")\n",
    "print(f\"Predicted Emotion: {emotion}\")\n",
    "print(f\"Confidence: {conf:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed1e7fb-e66b-414d-822d-e6c073316e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
